#+LATEX_HEADER: \usepackage[backend=biber, authordate, ibidtracker=context,natbib,doi=false,isbn=false,url=false]{biblatex-chicago}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \addbibresource{~/Documents/bibliography/references.bib}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}
# #+LATEX_HEADER: \onehalfspacing
#+OPTIONS: toc:nil num:t
#+TITLE: What Bayesian Models of Cognition Tell Us About the Problem of Induction
#+AUTHOR: Conrad Friedrich
#+DATE: \today

#+BEGIN_abstract

How does the mind make so much from so little?

This catchy rephrasing of the classical problem of induction is the central theme of the present paper. It is argued that human inductive reasoning is successful because of the copious use of inductive biases. The inductive constraints are learned in the form of rich, structured and abstract background knowledge. A key factor is the ability to transfer structured knowledge between different learning problems. Bayesian models of cognition are empirically adequate and capture the hierarchical structure of background knowledge convincingly. 

thesis: Examining human inductive reasoning sheds light on the problem of induction by giving an account of how inductive biases can be acquired in a motivated, systematic fashion. 

Thesis: Hierarchical Bayesian Models give a convincing account of biases in many human learning scenarios. Since humans are arguably very successful reasoners in many situations, we get a prima facie reason as to how inductive biases could/should be structured to be successful.  

The argument proceeds as follows. Human inductive reasoning is arguably successful. Results from statistical learning theory suggest that there is no free lunch, i.e. no universal learning procedure. Instead, inductive biases provide reasonable limitations for the hypothesis space. Bayesian models of human cognition, and in particular, hierarchical models give a convincing explanation for the structure and the origin of such biases. By examining these models closely, we can gain insight into how the biases are formed and updated and how they contribute to successful inductive inference. 
#+END_abstract



* Introduction    



* TODO COMPLETELY REWRITE ABSTRACT ONCE DONE W PAPER 

The problem of induction has been historically pervasive. Starting with Hume's famous argument for the claim that there is no hope for justification of inferences which go beyond the observed data citep:hume39_treat_human_natur,hume48_enquir_concer_human_under,  the problem reemerged multiple times in different guises in philosophy, e.g. in Goodman's new riddle of induction citep:goodman55_fact_fiction_forec. In statistical learning theory citet:wolpert96_lack_prior_distin_between_learn_algor is said to have reformulated the age-old problem in formally precise terms in form of the so-called No Free Lunch theorem. Since almost all reasoning in the artificial intelligence, human cognition, and even the sciences is fundamentally inductive, the problem continues to be relevant. Many authors citep:vapnik95_natur_statis_learn_theor suggest that a key to successful inductive inference lies in appropriately limiting the hypotheses under consideration, i.e. making use of inductive /biases/, /constraints/, or /priors/ citep:griffiths09_connec. 

The Bayesian framework has already been employed to address the problem of induction, perhaps most notably in the project of inductive logic citep:carnap50_logic_found_probab. Here too, however, the need for empirical premises in the form inductive biases became apparent citep:henderson18_probl_induc.
While probability theory and logic plausibly can provide an /a priori/ justification for optimal reasoning processes, the framework is compatible with any arbitrary conclusion, given the right premises.

Studying inductive biases, then, goes a long way in addressing key issues in the problem of induction. By taking a closer look at successful inductive reasoners and learning about their inductive biases, we might infer information about the structure of inductive biases more generally, if not at least get a good idea of how inductive biases might be acquired and applied. Actual human reasoning is arguably very successful, and the field of computational cognitive science builds mathematical models to explain this success citep:tenenbaum11_how_to_grow_mind,griffiths10_probab_model_cognit.

In this paper, we are particularly concerned with the /hierarchical structure/ of models in cognitive science. We examine how the structure of these models imposes certain constraints on the learner which induce systematic inductive biases and give a plausible account of how the biases are acquired. These models give a promising candidate as to how one /should/ reason inductively, thereby addressing the normative problem of justifying induction\footnote{We don't intend to commit an is-ought-fallacy though, which is why these models are only seen as a candidate for ideal reasoning. We don't claim this solves any inductive problem.}. 

While there may be no universal learning algorithm, bayesian models of cognition indicate a clear view on what 'optimal' bias could look like. By employing hierarchical representations, information is learned on different levels of abstraction. A single learning problem can inform different levels of abstraction, and abstract knowledge can be transferred to other learning problems, providing biases for future learning problems.

The paper proceeds as follows. 

** TODO ADD SIGNPOSTING TEXT (1 paragraph)

* Hierarchical Bayesian Models of Cognition

** TODO QUICK SIGNPOSTING

How does the mind get so much from so little? Ask citet:tenenbaum11_how_to_grow_mind pointedly. Their research focuses on issues in learning and inductive inferences in human cognition, paying particular attention to the human mind's ability to generate a rich, structured representation of the world that goes far beyond the limited data available to the learner. For example, children learn the application of words a lot faster and with fewer examples than a learner which considers all logically possible hypothesis, indicating that they employ strong inductive biases limiting the hypotheses under consideration. A key insight is that the hypotheses are constrained by abstract knowledge the learner applies to the problem. There are similarities between different learning problems a learner encounters in her development. These similarities can often be represented in the form of more abstract information than the mere data a problem provides. Learning about one problem, then, also informs the more abstract level, and facing another, slightly similar problem afterwards, the learner transfers her new found knowledge in the form of inductive bias. 

While many of the models mentioned above have hierarchical structure, all of them are modeled in the Bayesian framework. Bayesian modeling is a particular, wildly popular way to formally deal with reasoning under uncertainty, though by no means the only or only popular alternative citep:halpern03_reason_about_uncer. Bayesian models tend to be semantically transparent and readily interpretable. The Bayesian framework as a means of representing mental states and processes yields a symbolic system, as opposed to subsymbolic accounts, e.g. connectionism citep:clark00_mindw.  

Hierarchical Bayesian models (HBM) have been applied to a lot of different learning scenarios, and are generally found to agree with empirical data. That is, cases of actual human reasoning can be modeled adequately within the framework in a wide range of circumstances. 

Of course, the adequacy of the framework is not without its critics in cognitive science citep:mcclelland10_approac_lettin, as is its primacy in philosophy citep:colombo16_bayes_cognit_scien_monop_neglec_framew but this discussion leads too far afield for the purposes of this paper.

* Modeling Inductive Biases

** TODO Quick Signposting
[fn::The present section draws on citet:kruschke11_doing_bayes, chapters 5 and 9, citet:jaynes03_probab_theor, chapter 6, citet:gelman13_bayes_data_analy_third_edition, chapter 5, and reproduces a model of citet:kemp07_learn_overh_with_hierar_bayes_model.]

** The Simplest Bayesian Model

For the purposes of highlighting different model structures, we take a look at one of the most simple cases of Bayesian inferences. Hierarchical models can get quite complicated, so that the simplest model sufficient to make the point will be used. Following that, we will look at a slightly more complicated structure.

Consider the oft-used case of estimating the underlying parameter of a repeatable experiment with dichotomous outcomes. For example, we repeatedly draw marbles from a bag. We know there are only two different types of marbles, say blue and white, in the bag. Let's denote the proportion of white marbles in the bag as \(\theta \in (0,1)\), which is also the probability to draw a white marble. Given data /y/, observed draws /N/ with /z/ white marbles, what is our posterior subjective probability about the proportion? To calculate, we employ Bayes theorem:

\begin{equation}
  p(\theta|y) = \frac{p(y|\theta) p(\theta)}{p(y)}
\end{equation}

where 

\begin{equation}
p(y) = \int p(y|\theta')p(\theta')d\theta'.
\end{equation}

We may plausibly assume each draw generated by a Bernoulli distribution, hence the likelihood $p(y|\theta)$ is given by 

\begin{equation}
p(y|\theta) =\binom{N}{z} \theta^z (1-\theta)^{N-z}.
\end{equation}

Lastly, \( p(\theta) \) represents our prior belief of the proportion of white marbles. In the Bayesian framework, the background knowledge the learner applies to the problem is represented by the prior belief. The inductive bias of a learner can be modeled as the prior belief. For the current example, we assume a prior biased towards uniformity of the bags, as can be seen in figure [[fig:simplebayes]], top. Formally, we say that \theta is beta distributed with parameters $a,b$: 

\begin{equation}
\theta \sim ~ \text{Beta}(a,b)
\end{equation} 

and set $a=b=0.5$. Note that this is an arbitrary choice. In the Bayesian framework, we could use any kind of prior, as long as it is a probability distribution.

#+NAME: fig:simplebayes
#+CAPTION: Plots of the model described in section [[The Simplest Bayesian Model]]. Expected values of the posteriors plotted as a straight line. Labels for the y-axis omitted.
[[./SimpleBayes.pdf]]

Suppose we draw a single white ball and update our beliefs. The resulting posterior is plotted in figure [[fig:simplebayes]], center. The confidence has shifted from previously high confidence in an all-white and all-blue bag to just high confidence in an all-white bag. All other proportions are still on the table, however. This posterior is still vague.

After observing twenty draws of which 17 have been white, the resulting posterior is a lot more certain, plotted in figure [[fig:simplebayes]], bottom. The data has had considerable impact on the posterior, while prior belief does not have much effect. Almost all confidence lies between 0.6 and 1.0. Note that the previously high confidence for an all-blue bag is gone. Pressed for a point estimate of the probability that the next draw is a white marble, the Bayesian reasoner might give the expected value of the posterior distribution, plotted as a straight line. 

This straightforward problem is addressed neatly in the Bayesian framework.

** Multiple Parameters

Consider now a case where you encounter a whole stack of bags of marbles. We open up several bags and find mixed quantities of blue and white marbles. 
What can we predict for subsequent draws? 

Arguably, the probability of colors drawn from each bag is determined by the proportion of colors in each bag, and hence and appropriate model has multiple parameters \( \theta_i \), one for each bag /i/. Since each bag is different, our prior assumes the bags proportions to be independent, formally \({ p(\theta_i) = p(\theta_i|\theta_j) }\) for all \(i,j\). We assume the same prior as before, such that each \(\theta_i \sim \text{Beta}(a,b) \) with \(a=b=0.5\). Each \(\theta_i\) is individually estimated by the marbles we drew out of that bag /i/.

Suppose now that we examine 20 bags, of which we draw 20 marbles each. The results are varied, with the average proportion of white marbles in a bag tending towards less than \(0.5\). When we decide to open a 21st bag and draw a white marble, what is the posterior estimate for the proportion in that bag, i.e. \( p(\theta_{21}) \)? It is the same as in the case with only one bag, with \( N=1, z=1 \), figure [[fig:simplebayes]], center. We haven't learned anything about bag 21 by looking at any of the other bags, per assumption of the model.  

This seems unproblematic, so far. Compare, however, your intuition in the following case:

- The Curious Uniform Marble Case :: You encounter an abandoned stack of bags of marbles, and, curiously, start emptying one after the other. After 20 bags, all of them have been completely uniform in color: 10 have been all-blue, 10 have been all-white. You open the 21st, and draw a white marble. What color do you expect the rest of the marbles in the bag to be? 

The intuition is clear, we claim: We have good reason to assume the rest of the marbles to be white, that is, place high confidence on an all-white bag. More confidence, at least, than would the 21st bag have been the first bag to open. This intuition is key and we'll address it again in a bit. First, let us look at what our simple model with multiple parameters suggests, as can be seen in figure [[fig:flat20]].  

#+CAPTION: Plots from the model described in section [[Multiple Parameters]]. Each row shows the distributions of a single parameter given different data, here \(\theta_1, \theta_{11}, \theta_{21}\). The first column shows the priors. The second column shows the posteriors after mixed input, where \(N_1 = 20, z_1 = 1, N_{11} = 20, z_{11} = 6, N_{21} = 1, z_{21} = 1\). The third column shows the posteriors after observing the uniform bags as, where \(N_1 = 20, z_1 = 20, N_{11} = 20, z_{11} = 0, N_{21} = 1, z_{21} = 1\). 
#+Name: fig:flat20
[[./Flat20Bags.pdf]] 

The first two rows show \(\theta_1, \theta_{11}\). Each bag got 20 draws, with different posteriors dependent on the number of white marbles. The third row shows \( \theta_{21} \), which we estimate after only a single draw as in Uniform marble case. Notably, both posteriors distributions are identical (i promise) The third row shows \( \theta_{21} \), which we estimate after only a single draw. Notably, both posteriors distributions are identical. They are also identical to the case of a single parameter as described in section [[The Simplest Bayesian Model]]. That is, these models do not make any difference between the situations as far as \( \theta_{21} \) is concerned. The clear intuition just described suggests that we have stronger confidence in the next marble drawn from bag 21 being white in the uniform case than in the mixed bag case. The model as presented cannot account for this intuition.

How could we as conservatively as possible deal with this mismatch of intuition and model prediction? 

First, by denying the challenge. If you don't share the intuition, the model doesn't need to account for it, right? But some examination reveals this is not so easy. Assuming we actually want intuitive reasoning and our model to agree, it is absolutely natural to assume /some/ similarity between the bags, given that we found them all on a stack. They already share some causal history, which makes it all the more plausible they share some other features, too. Even the slightest nod in this direction renders the model inadequate. This does not lead anywhere for the committed Bayesian. 

Second, we might make a methodological point: This isn't a question of intuition at all, aren't we arguing from cognitive science? This is a valid question and will be addressed later on in section [[Objections]]. We argue against it, however.

Third, we might adapt the model to fit. This is much more promising. How to proceed? We want the marbles from the other bags to inform our estimate of \theta_{21}. A natural response is to take them into account, too, and to condition on all drawn marbles from all 21 bags. But this shifts all of the import on the cumulative data. Whether we draw a white or blue marble from bag 21 only shifts our posterior minutely. And shouldn't the marble from the actual bag we are examining be relevant to our estimation? More subtly, we might give a weighted average, and define weights \(w_1, w_2\) such that our posterior estimate is a linear convex combination of the estimations of bag number 21 and all other bags:
\begin{equation}
p^*(\theta_{21}) = w_1 p(\theta|y_{21}) + w_2 p(\theta|y_{1\dots 20})
\end{equation} 
where \(y_{1\dots 20}\) denotes the data from all bags combined. This might yield desired results in some cases, as arguably in the mixed bag case, but suffers from two important defects: It fails for bimodal problems. Such a case is the uniform bags case. After observing 20 uniform bags of both colors equally often, we /should/ expect, before drawing, the next bag to be uniform in color too, without knowing the color yet. The average over all draws, however, shows no such bimodal tendency. Instead, the posterior will be very certain, that is, narrow, close to an estimate of 0.5 for \theta. No linear combination can fix that. But even if, with a lot of fidgeting, we could fine-tune this approach, we'd still be open to a philosophical sucker punch, since it'd be completely /ad-hoc/ and reverse engineered---from the desired solution backwards to a computational account of the reasoning process, without providing an explanation and instead introducing parameters just to make the calculation work. This is rather unsatisfying.

As hinted at some times in this paper already, hierarchical Bayesian models provide a neat solution. In the next section, we'll develop our model further into a simple hierarchical structure and show how the adjusted model can deal with the challenges.

** Introducing Hierarchy

Strictly speaking, the model discussed so far has a hierarchy: We take the observable data generation to be influenced by a parameter \theta we cannot directly observe. Instead, we estimate the parameter. In a hierarchical models, we just add more of these latent variables: We take to parameter \theta to be influenced by additional parameters. Such structures of probabilistic dependence and independence combined with probability distributions over them can represent abstract knowledge (e.g. cite:goodman10_learn_theor_causal,kemp09_struc_statis_model_induc_reason). For example, we might learn in the uniform bags case that the bags tend to be uniform in color, but that it is not clear whether blue or white. This abstract knowledge can be represented by a joint probability distribution over higher level parameters citep:kemp07_learn_overh_with_hierar_bayes_model, as we will describe and examine in this section.

As before, we observe 21 bags, denoted \(y_i\), with parameters \(\theta_i\). Now, instead of priors with fixed parameters for theta, we model the parameters, too. Figure \ref{fig:bayesnet} shows the independency structure.    

\begin{figure}[ht]
  \begin{center}
    \begin{tabular}{cc}

    \begin{tikzpicture}

  \node[obs]                     (y) {$y_i$};
  \node[latent, above=of y] (t) {$\theta_i$};
  \node[latent, above=of t]  (a) {$\alpha$};
  \node[latent, right=of a]  (b) {$\beta$};

  \edge {t} {y};
  \edge {a,b} {t} ; 

  \end{tikzpicture}

    \end{tabular}
  \end{center}
  
  \caption{\label{fig:bayesnet} Dependencies intended in the hierarchical model, here in form of a directed acyclic graph.}
\end{figure}

In addition to figure \ref{fig:bayesnet}, the model is given by:

\begin{align*}
  y_i &\sim \text{Bin}(\theta) \\
  \theta &\sim \text{Beta}(a,b),  \\
  a &= \alpha\beta, \\ 
  b &= \alpha(1-\beta) \\ 
  \alpha &\sim \text{Exp}(1) \\
  \beta &\sim \text{Beta}(1,1) \\
\end{align*}

The parameters \alpha and \beta describe how we may think the \(\theta_i\) are distributed, by way of a beta distribution with parameters /a,b/. They influence all \(\theta_i\). By learning more about a single \theta, we may shift our confidence about the generating parameters \alpha and \beta. This, in turn, influences our beliefs about different \theta.

Formally, given this graph to constrain our probability distribution, we can apply the Markov condition to the chain rule and simplify the calculation for the posterior joint distribution, for \(n=21\):

#+NAME: eq:jointposterior
\begin{equation}
p(y_1,\dots,y_n,\theta_1,\dots,\theta_n,\alpha,\beta) = p(\alpha)p(\beta)\prod_{i=1}^n p(y_i|\theta_i)p(\theta_i|\alpha,\beta)
\end{equation}

#+NAME: fig:hierarchical20 
#+CAPTION: Plots from the model described in section [[Introducing Hierarchy]]. Each row shows the distributions of a single parameter given different data, here \(\theta_{11}, \theta_{21}\). The first column shows the priors. The second column shows the posteriors after mixed input, where \(N_{11} = 20, z_{11} = 6, N_{21} = 1, z_{21} = 1\). The third column shows the posteriors after observing the uniform bags as, where \(N_{11} = 20, z_{11} = 0, N_{21} = 1, z_{21} = 1\). 
[[./Hierarchical20Bags.pdf]]


In the end, we are mostly interested in the posterior distribution of the \(\theta_i\) after learning all data. We can calculate the marginal posterior for, say, \theta_1 by conditioning equation [[eq:jointposterior]] on \(y\) and integrating out all other parameters. 

For even slightly complicated models like this one, the equation doesn't usually admit to an analytical solution, such that we need to apply a numerical strategy. With a straight forward grid approximation this can become quite time intensive to compute with a growing number of parameters as the number of point to calculate explodes, at least before optimization. In recent decades, however, a family of algorithms have been developed to address these issues. These so-called Markov Chain Monte Carlo (MCMC)[fn::A very accessible introduction can be found e.g. in citet:kruschke11_doing_bayes, chapter 7.] algorithms have been employed to calculate the posteriors in figure [[fig:hierarchical20]]. 

Two things should be noted about the results. First, although the priors on \theta are identical between this model and the model in section [[Multiple Parameters]], the posteriors differ. In the uniform case, for \theta_1 to \theta_{20}, the hierarchical model allows the posteriors to be more opinionated and more certain. This is a result of the abstract learned knowledge that the bags tend to be uniform in color, resulting in high confidence that the rest of the marbles drawn from any bag will be of that same color, too. 

Second, unlike the model in section [[Multiple Parameters]], the posterior of \theta_{21} differs between both cases different in mixed- and uniform case. Arguably in both cases, the estimate is a lot better: In particular, the result in the uniform case shows a strong tendency to expect the next marble out of \theta_{21} to be white, in accordance with the intuition about the uniform marble case.

** Forming Inductive Bias

To put it a bit more generally, a big selling point of HBMs is that they enable the learner to /transfer/ learned knowledge from one learning problem to another. To be sure, in the presented example case, there are other Bayesian options to account for the transfer of knowledge about the proportion of marbles from one bag to the next, for example by introducing more complicated models on the base level. HBMs, though, give a systematic account of how such a transfer can, in principle, be achieved. And they do so for arbitrary levels of abstraction. 

By providing this ability to transfer, HBMs give an astonishingly natural account of how inductive bias for a particular learning problem is acquired: by recognizing similarity between problems and transferring learned bias from an old problem to a new one. 

The attentive philosopher will note: This requires an inductive bias on a higher level, namely that similar problems should be addressed similarly, hence that knowledge on the object level is transferable. Doesn't this just push the problem of justifying inductive bias to a higher level, thereby not solving, but instead just displacing the problem of induction? We claim that is exactly what is happening, but that it is not a vice, but a virtue. It is similar to the prominent advantage of Bayesian models of for statistical purposes. They force us to be explicit about the prior information we bring into an inference problem. Similarly, HBMs make explicit the abstract knowledge involved in inductive reasoning. This way, Bayesian statistics just as HBMs in cognitive science open up their respective research problems to a whole new range of precise analysis. For the philosopher, too, HBMs open up the possibility to precisely analyze the inductive biases implicit in inductive reasoning problems, and may in this manner shed light on aspects of the bias that haven't researched yet. So, definitely worth our time! 

** Hierarchical Bayesian Models of Cognition

The example model given above only shows that HBMs /can/ give a convincing account of /some/ abstract knowledge in a reasoning process. But this is exactly what we set out to do: Highlight HBMs as a possible way to model abstract knowledge. Of course, these cases here are over simplified. Rarely is data just binary without predictor variables. The levels of abstraction are trivial here, and amount to almost daunting proportions in realistic scenarios. To argue the claim that HBMs are a plausible model for many, if not almost all, cases of inductive reasoning with abstract knowledge is a whole research field. Helpful overviews are given by, e.g. citet:tenenbaum06_theor_based_bayes_model_induc_learn_reason,griffiths10_probab_model_cognit,tenenbaum11_how_to_grow_mind. HBMs are extremely flexible and can account for very diverse learning problems, as has been noted in the introduction. They can account for how such abstract knowledge is learned and formed, too citep:kemp10_probab_model_theor_format.

* Objections
** How Does Adding Parameters Reduce Complexity?

Making a model more complicated by adding additional parameters runs counter to the idea of introducing inductive bias, as these are concepts that are usually seen to be in opposition to one another, as exemplified in the bias-complexity-tradeoff problem (e.g. cite:shalev-shwartz14_under_machin_learn,harman07_reliab_reason). Instead, we might run the risk of overfitting. HBMs are undeniably more complicated than simple, flat models. We give two reasons that they nevertheless can still plausibly account for inductive biases in the intended sense. 

First, HBMs create a certain effect, called /shrinkage/ in the statistical literature citep:kruschke11_doing_bayes. In our bag example, the estimation of each \( \theta_i \) tends more towards the average than the data would suggest on its own, shrinking, in general, the range of estimations. This effect naturally reduces overfitting.

Second, the model naturally applies to more data than a single model would. In the bag example, we treat all data within the same model with a lot of parameters, instead of having a separate model for each bag. Additionally, by also modeling more abstract knowledge, the model is open for more abstract data: receiving information that stacks of back tend to have the same characteristics, for example, can inform our model as higher-level data, so to speak. By accounting for more data, the risk to overfit is considerably lowered, as cite:perfors11_tutor_introd_to_bayes_model_cognit_devel, section 4, address.  

Taken together, by applying the same, more complex model to more situations and hence account for more data, hierarchical models can induce biases instead of increasing complexity.

** Human Reasoning Does not Guarantee Rational Reasoning

Perhaps it needs to be added that the claim in this paper is /not/ meant to imply that we just look at how humans reason, model is hierarchically, and voila, solve the problem of induction. /non sequitur/ aside, it is provably very often the case that humans do not reason rationally. We might even find that most human reasoning is not optimal. Instead, we want to look at specific cases that highlight the human mind's extraordinary ability to generalize successfully from sparse data, as described in the beginning.  

* Conclusion

By giving a systematic account of how inductive bias can be acquired and plausibly is successfully acquired in many human learning situations, HBMs inform how, reasonably, abstract background knowledge can be applied to inductive inference problems. While providing no obvious solution to Hume`s original problem of induction, we certainly gain insight into rationally structured inductive bias in human reasoning, and, by extension, into the nature of bias in learning problems.

* Appendix

The hierarchical models described in this paper and the results presented have been implemented with the programming language R and a sampling framework called JAGS, which implements a type of Gibbs sampling Markov Chain Monte Carlo. Some minor auxiliary functionality was used from the accompanying program code of citet:kruschke11_doing_bayes. All source code and the initial parameter for the data generation used can be looked up at the time of writing at the author's git repository under 
https://github.com/kurtfritz/bayesian-cognition.

\printbibliography

