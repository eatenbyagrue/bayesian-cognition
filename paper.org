#+LATEX_HEADER: \usepackage[backend=biber, authordate, ibidtracker=context,natbib,doi=false,isbn=false,url=false]{biblatex-chicago}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \addbibresource{~/Documents/bibliography/references.bib}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}
#+LATEX_HEADER: \onehalfspacing
#+OPTIONS: toc:t num:t
#+TITLE: What Bayesian Models of Cognition Tell Us About the Problem of Induction
#+SUBTITLE: 4598 words
#+AUTHOR: Conrad Friedrich
#+DATE: \today
\thispagestyle{empty}

\newpage

* Introduction    

In this paper we argue that hierarchical Bayesian models (HBMs) as used in describing human inference processes in the cognitive sciences can inform the philosophical discussion on the problem of induction. In the cognitive sciences, HBMs bring the virtues of precise formal analysis in the Bayesian framework to bear on the research modeling biases in inductive inference processes. They force the researcher to be explicit and precise about modeling the learner's abstract background knowledge. Bayesian models of human cognition, and in particular, hierarchical models give a convincing explanation for the structure and the origin of such inductive biases. By examining these models closely, we can gain insight into how the biases are formed and updated and how they contribute to successful inductive inference. 

In many situations, human reasoning is an exemplar of successful generalization. By examining plausible descriptive formal models of inductive bias in successful reasoning, the philosophical discussion can benefit by identifying key conditions of a successful inductive inference. We argue that by looking at HBMs to model inductive bias, we are able to identify such key conditions. The argument in the paper proceeds as follows:  

1. Human inductive reasoning is successful in relevant cases.
2. HBMs give an empirically adequate account of bias in inductive reasoning processes.
3. HBMs /can/ highlight structural features of the reasoning process that enable successful inductive inference.

From this, we can establish the claim of this paper.

For premises 1 and 2, we survey and have to refer to and describe the relevant empirical literature. These are less controversial. The main part of this paper concerns itself with providing support for premise 3. We do so by---step by step---building up a more and more complex model of a simple reasoning process, intending to make transparent the benefits that come with explicitly modeling inductive bias in the form of abstract background knowledge.

The problem of induction has been historically pervasive. Starting with Hume's famous argument for the claim that there is no hope for justification of inferences which go beyond the observed data citep:hume39_treat_human_natur,hume48_enquir_concer_human_under,  the problem reemerged multiple times in different guises in philosophy, e.g. in Goodman's new riddle of induction citep:goodman55_fact_fiction_forec. In statistical learning theory, citet:wolpert96_lack_prior_distin_between_learn_algor is said to have reformulated the age-old problem in formally precise terms in form of the so-called No Free Lunch theorem. Since reasoning in artificial intelligence, human cognition, and even the sciences is fundamentally inductive, the problem continues to be relevant. Many authors (e.g. citet:vapnik95_natur_statis_learn_theor) suggest that a key to successful inductive inference lies in appropriately limiting the hypotheses under consideration, that is, making use of inductive /biases/, /constraints/, or /priors/ citep:griffiths09_connec. 

The Bayesian framework has already been employed to address the problem of induction, perhaps most notably in the project of inductive logic citep:carnap50_logic_found_probab. Here too, however, the need for empirical premises in the form inductive biases became apparent citep:henderson18_probl_induc.
While probability theory and logic plausibly can provide an /a priori/ justification for optimal reasoning processes, the processes are compatible with any arbitrary conclusion, given the right premises.

Studying inductive biases, then, goes a long way in addressing the problem of induction. By taking a closer look at successful inductive reasoners and learning about their inductive biases, we might infer information about the structure of inductive biases more generally, if not at least get a good idea of how inductive biases might be acquired and applied. Actual human reasoning is arguably very successful, and the field of computational cognitive science builds mathematical models to explain this success citep:tenenbaum11_how_to_grow_mind,griffiths10_probab_model_cognit.

In this paper, we are particularly concerned with the /hierarchical structure/ of models in cognitive science. We examine how the structure of these models imposes certain constraints on the learner which induce systematic inductive biases and give a plausible account of how the biases are acquired. 

While there may be no universal learning algorithm, bayesian models of cognition indicate what optimal bias could look like. By employing hierarchical representations, information is learned on different levels of abstraction. A single learning problem can inform different levels of abstraction, and abstract knowledge can be transferred to other learning problems, providing biases for future learning problems.

The paper is structured as follows. In section [[Hierarchical Bayesian Models of Cognition]], we survey the literature in the cognitive sciences and establish the success of HBMs in empirical models. Section [[Modeling Inductive Biases]] is the main part of the paper and contains the argument for premise 3 as described in this section. We develop a hierarchical model for a simple inference problem from the ground up and look closely at its features. We identify central features of this approach that contribute to modeling inductive bias with HBMs and emphasize how they support the claim of the paper. In section [[Objections]], finally, we look at some glaring objections and argue against them. 

* Hierarchical Bayesian Models of Cognition

In this section, we survey the literature in the cognitive science and establish the success of HBMs in empirical models.

How does the mind get so much from so little? Ask citet:tenenbaum11_how_to_grow_mind pointedly. Their research focuses on issues in learning and inductive inferences in human cognition, paying particular attention to the human mind's ability to generate a rich, structured representation of the world that goes far beyond the limited data available to the learner. For example, children learn the application of words a lot faster and with fewer examples than a learner which considers all logically possible hypothesis, indicating that they employ strong inductive biases limiting the hypotheses under consideration. A key insight is that the hypotheses are constrained by abstract knowledge the learner applies to the problem. There are similarities between different learning problems a learner encounters in her development. These similarities can often be represented in the form of more abstract information than the mere data a problem provides. Learning about one problem, then, also informs the more abstract level, and facing another, slightly similar problem, the learner transfers her newly found knowledge in the form of inductive bias. 

While many of the models mentioned above have hierarchical structure, even more are modeled in the Bayesian framework. Bayesian modeling is a particular, wildly popular way to formally deal with reasoning under uncertainty, though by no means the only or only popular alternative citep:halpern03_reason_about_uncer. Bayesian models tend to be semantically transparent and readily interpretable. The Bayesian framework as a means of representing mental states and processes yields a symbolic system, as opposed to subsymbolic accounts, notably connectionism citep:clark00_mindw.  

Hierarchical Bayesian models (HBM) have been applied to a lot of different learning scenarios, and are generally found to agree with empirical data. That is, cases of actual human reasoning can be modeled adequately within the framework in a wide range of circumstances[fn::Many papers cited in this paper provide evidence for this claim.].

What are the elements of the hierarchy? On different levels of the hierarchy are different types of hypotheses. For example, when modeling language comprehension, we might use parse trees to model individual sentences. One level higher, they can be explained by grammars, which in turn might be explained by recourse to Universal Grammar citep:kemp07_learn_overh_with_hierar_bayes_model. We find an explanatory relation between hypotheses on different levels of the hierarchical model. By additionally supplying probability distributions with appropriate parameters, this hierarchical structure is amenable to Bayesian analysis.  

Of course, the adequacy of the framework is not without its critics in cognitive science citep:mcclelland10_approac_lettin and in philosophy of cognitive science citep:colombo16_bayes_cognit_scien_monop_neglec_framew, but this discussion is more general and leads too far afield for the purposes of this paper.

* Modeling Inductive Biases

This section presents an extendend argument for premise 3 as described in the introduction.

- HBMs /can/ highlight structural features of the reasoning process that enable successful inductive inference.

The section is structured as follows. We first build a simple Bayesian model in section [[The Simplest Bayesian Model]] and extend it to a slightly more complex Bayesian model in section [[Multiple Parameters]]. Recognizing and highlighting its shortcomings, we develop a hierarchical Bayesian model that addresses these problems in section [[Introducing Hierarchy]]. We examine closely /why/ this is successful and identify structural features that contribute to its success[fn::The present section draws on citet:kruschke11_doing_bayes, chapters 5 and 9, citet:jaynes03_probab_theor, chapter 6, citet:gelman13_bayes_data_analy_third_edition, chapter 5, and reproduces a model of citet:kemp07_learn_overh_with_hierar_bayes_model.].
** The Simplest Bayesian Model

For the purposes of highlighting different model structures, we take a look at one of the simplest cases of Bayesian inferences. Following that, we will look at a model with a slightly more complicated structure.

Consider the oft-used case of estimating the underlying parameter of a repeatable experiment with dichotomous outcomes. For example, we repeatedly draw marbles from a bag. We know there are only two different types of marbles, say blue and white, in the bag. Let's denote the proportion of white marbles in the bag as \(\theta \in (0,1)\), which is also the probability to draw a white marble at random. Given data /y/, observed draws /N/ with /z/ white marbles, what is our posterior subjective probability about the proportion? To calculate, we employ Bayes theorem:

\begin{equation}
  p(\theta|y) = \frac{p(y|\theta) p(\theta)}{p(y)}
\end{equation}

where 

\begin{equation}
p(y) = \int p(y|\theta')p(\theta')d\theta'.
\end{equation}

We may plausibly assume each draw generated by a Bernoulli distribution, hence the likelihood $p(y|\theta)$ is given by 

\begin{equation}
p(y|\theta) =\binom{N}{z} \theta^z (1-\theta)^{N-z}.
\end{equation}

Lastly, \( p(\theta) \) represents our prior belief of the proportion of white marbles. In the Bayesian framework, the background knowledge the learner applies to the problem is represented by the prior belief. The inductive bias of a learner can be modeled as the prior belief. For the current example, we assume a prior biased towards uniformity of the bags, as can be seen in figure [[fig:simplebayes]], top. Formally, we say that \theta is beta distributed with parameters $a,b$: 

\begin{equation}
\theta \sim ~ \text{Beta}(a,b)
\end{equation} 

and set $a=b=0.5$. Note that this is an arbitrary choice. In the Bayesian framework, we could use any kind of prior as long as it is a probability distribution.

#+NAME: fig:simplebayes
#+ATTR_LATEX: :width 1\linewidth 
#+CAPTION: Plots of the model described in section [[The Simplest Bayesian Model]]. Expected values of the posteriors plotted as a straight line. Labels for the y-axis omitted.
[[./SimpleBayes.pdf]]

Suppose we draw a single white ball and update our beliefs. The resulting posterior is plotted in figure [[fig:simplebayes]], center. The confidence has shifted from previously high confidence in an all-white and all-blue bag to just high confidence in an all-white bag. All other proportions of marbles in the bag are still on the table, however. This posterior is still /vague/.

After observing twenty draws of which 17 have been white, the resulting posterior is a lot more /certain/, plotted in figure [[fig:simplebayes]], bottom. The data has had considerable impact on the posterior, while the prior belief does not have much effect. Almost all confidence lies between 0.6 and 1.0. Note that the previously high confidence for an all-blue bag is gone. Pressed for a point estimate of the probability that the next draw is a white marble, the Bayesian reasoner might give the expected value of the posterior distribution, plotted as a straight line. 

This straightforward problem is addressed neatly in the Bayesian framework.

** Multiple Parameters

Consider now a case where you encounter a whole stack of bags of marbles. We open up several bags and find mixed quantities of blue and white marbles. 
What can we predict for subsequent draws? 

Arguably, the probability of colors drawn from each bag is determined by the proportion of colors in each bag, and hence and appropriate model has multiple parameters \( \theta_i \), one for each bag /i/. Since each bag is different, our prior assumes the bags proportions to be independent, formally \({ p(\theta_i) = p(\theta_i|\theta_j) }\) for all \(i,j\). We assume the same prior as before, such that each \(\theta_i \sim \text{Beta}(a,b) \) with \(a=b=0.5\). Each \(\theta_i\) is individually estimated by the marbles we drew out of that bag /i/.

Suppose now that we examine 20 bags, of which we draw 20 marbles each. The results are varied, with the average proportion of white marbles in a bag tending towards less than \(0.5\). When we decide to open a 21st bag and draw a white marble, what is the posterior estimate for the proportion in that bag, i.e. \( p(\theta_{21}) \)? It is the same as in the case with only one bag, with \( N=1, z=1 \), figure [[fig:simplebayes]], center. We haven't learned anything about bag 21 by looking at any of the other bags, per assumption of the model.  

This seems unproblematic, so far. Compare, however, your intuition in the following case:

- The Curious Uniform Marble Case :: You encounter an abandoned stack of bags of marbles, and, curiously, start drawing from one after the other. After 20 marbles each from 20 bags, all of the marbles have been completely uniform in color: 10 have been all-blue, 10 have been all-white. You open the 21st, and draw a white marble. What color do you expect the rest of the marbles in the bag to be? 

The intuition is clear, we claim: We have good reason to assume the rest of the marbles to be white, therefore we place high confidence on an all-white bag. More confidence, at least, than would the 21st bag have been the first bag to open. This intuition is key. Let us look at what our simple model with multiple parameters suggests, as can be seen in figure [[fig:flat20]].  

#+CAPTION: Plots from the model described in section [[Multiple Parameters]]. Each row shows the distributions of a single parameter given different data, here \(\theta_1, \theta_{11}, \theta_{21}\). The first column shows the priors. The second column shows the posteriors after mixed input, where \(N_1 = 20, z_1 = 1, N_{11} = 20, z_{11} = 6, N_{21} = 1, z_{21} = 1\). The third column shows the posteriors after observing the uniform bags as, where \(N_1 = 20, z_1 = 20, N_{11} = 20, z_{11} = 0, N_{21} = 1, z_{21} = 1\). 
#+Name: fig:flat20
#+ATTR_LATEX: :width 1.1\linewidth 
[[./Flat20Bags.pdf]] 

The first two rows show \(\theta_1, \theta_{11}\). Each bag number 1--20 got 20 draws, with different posteriors dependent on the number of white marbles. The third row shows \( \theta_{21} \), which we estimate after only a single draw as in uniform marble case. Notably, both posteriors distributions are identical. They are also identical to the case of a single parameter as described in section [[The Simplest Bayesian Model]]. That is, these models do not make any difference between the cases as far as \( \theta_{21} \) is concerned. The clear intuition just described suggests that we have stronger confidence in the next marble drawn from bag 21 being white in the uniform case than in the mixed bag case. The model as presented cannot account for this intuition.

How could we as conservatively as possible deal with this mismatch of intuition and model prediction? 

First, by denying the challenge. If you don't share the intuition, the model doesn't need to account for it, right? But some examination reveals this is not so easy. Assuming we actually want intuitive reasoning and our model to agree, it is absolutely natural to assume /some/ similarity between the bags, given that we found them all on a stack. They already share some causal history, which makes it all the more plausible they share some other features, too. Even the slightest nod in this direction renders the model inadequate. This does not lead anywhere for the committed Bayesian. 

Second, we might make a methodological point: This isn't a question of intuition at all, aren't we arguing from cognitive science? This is a valid question and will be addressed later on in section [[Objections]]. We argue against it, however.

Third, we might adapt the model to fit. This is much more promising. How to proceed? We want the marbles from the other bags to inform our estimate of \theta_{21}. A natural response is to take them into account, too, and to condition on all drawn marbles from all 21 bags. But this shifts all of the import on the cumulative data. Whether we draw a white or blue marble from bag 21 only shifts our posterior minutely. And shouldn't the marble from the actual bag we are examining be relevant to our estimation? More subtly, we might give a weighted average, and define weights \(w_1, w_2\) such that our posterior estimate  \(p^*\) is a linear convex combination of the estimations of bag number 21 and all other bags:
\begin{equation}
p^*(\theta_{21}) = w_1 p(\theta|y_{21}) + w_2 p(\theta|y_{1\dots 20})
\end{equation} 
where \(y_{1\dots 20}\) denotes the data from all bags combined and \theta is a generic single parameter like in the simplest model. This might yield desired results in some cases, as arguably in the mixed bag case, but suffers from two important defects: (i) It fails for bimodal problems. Such a case is the uniform bags case. After observing 20 uniform bags of both colors equally often, we /should/ expect, before drawing, the next bag to be uniform in color too, without knowing the color yet. The average over all draws, however, shows no such bimodal tendency. Instead, the posterior will be very certain, that is, narrow, close to an estimate of 0.5 for \theta. No linear combination can fix that. But even if, with a lot of fidgeting, we could fine-tune this approach, we'd (ii) still be open to a philosophical sucker punch, since it'd be completely /ad-hoc/ and reverse engineered---from the desired solution backwards to a computational account of the reasoning process, without providing an explanation and instead introducing parameters just to make the calculation work. This is rather unsatisfying.

As hinted at some times in this paper already, hierarchical Bayesian models provide a way out. In the next section, we'll develop our model further into a simple hierarchical structure and show how the adjusted model can deal with the challenges.

** Introducing Hierarchy

Strictly speaking, the model discussed so far already has a hierarchy: We take the observable data generated by a parameter \theta which we cannot directly observe. Instead, we estimate the parameter. In hierarchical models, we just add more of these unobserved variables: We take parameter \theta to be influenced by additional parameters. Such structures of probabilistic dependence and independence combined with probability distributions over them can represent abstract knowledge (e.g. cite:goodman10_learn_theor_causal,kemp09_struc_statis_model_induc_reason). For example, we might learn in the uniform bags case that the bags tend to be uniform in color, but that it is not clear whether uniformly blue or white. This abstract knowledge can be represented by a joint probability distribution over higher level parameters citep:kemp07_learn_overh_with_hierar_bayes_model, as we will describe and examine in this section.

As before, we observe 21 bags, their data denoted \(y_i\), with parameters \(\theta_i\). Now, instead of priors with fixed parameters for theta, we model the parameters, too. Figure \ref{fig:bayesnet} shows the independency structure.    

\begin{figure}[ht]
  \begin{center}
    \begin{tabular}{cc}

    \begin{tikzpicture}

  \node[obs]                     (y) {$y_i$};
  \node[latent, above=of y] (t) {$\theta_i$};
  \node[latent, above=of t]  (a) {$\alpha$};
  \node[latent, right=of a]  (b) {$\beta$};

  \edge {t} {y};
  \edge {a,b} {t} ; 

  \end{tikzpicture}

    \end{tabular}
  \end{center}
  
  \caption{\label{fig:bayesnet} Dependencies intended in the hierarchical model, here in form of a directed acyclic graph.}
\end{figure}

In addition to figure \ref{fig:bayesnet}, the model is given by the following description. 

\begin{align*}
  y_i &\sim \text{Bin}(\theta) \\
  \theta &\sim \text{Beta}(a,b),  \\
  a &= \alpha\beta, \\ 
  b &= \alpha(1-\beta) \\ 
  \alpha &\sim \text{Exp}(1) \\
  \beta &\sim \text{Beta}(1,1) 
\end{align*}

The parameters \alpha and \beta describe how we may think the \(\theta_i\) are distributed, by way of a beta distribution with parameters /a,b/. They influence all \(\theta_i\). By learning more about a single \theta, we may shift our confidence about the generating parameters \alpha and \beta. This, in turn, influences our beliefs about different \(\theta_i\).

Formally, given the graph in figure \ref{fig:bayesnet} to constrain our probability distribution, we can apply the Markov condition to the chain rule from probability theory and simplify the calculation for the posterior joint distribution, for \(n=21\), to:

#+NAME: eq:jointposterior
\begin{equation}
p(y_1,\dots,y_n,\theta_1,\dots,\theta_n,\alpha,\beta) = p(\alpha)p(\beta)\prod_{i=1}^n p(y_i|\theta_i)p(\theta_i|\alpha,\beta)
\end{equation}

#+NAME: fig:hierarchical20 
#+CAPTION: Plots from the model described in section [[Introducing Hierarchy]]. Each row shows the distributions of a single parameter given different data, here \(\theta_{11}, \theta_{21}\). The first column shows the priors. The second column shows the posteriors after mixed input, where \(N_{11} = 20, z_{11} = 6, N_{21} = 1, z_{21} = 1\). The third column shows the posteriors after observing the uniform bags, where \(N_{11} = 20, z_{11} = 0, N_{21} = 1, z_{21} = 1\). 
#+ATTR_LATEX: :width 1.1\linewidth 
[[./Hierarchical20Bags.pdf]]


In the end, we are mostly interested in the posterior distribution of the \(\theta_i\) after learning all data. We can calculate the marginal posterior for, say, \theta_1 by conditioning equation [[eq:jointposterior]] on \(y\) and integrating out all other parameters. 

For even slightly complicated models like this one, the equation doesn't usually admit to an analytical solution, such that we need to apply a numerical strategy. With a straight forward grid approximation this can become quite time intensive to compute with a growing number of parameters as the number of points to calculate explodes, at least before optimization. In recent decades, however, a family of algorithms have been developed to address these issues. These so-called Markov Chain Monte Carlo (MCMC)[fn::A very accessible introduction can be found e.g. in citet:kruschke11_doing_bayes, chapter 7.] algorithms have been employed to calculate the posteriors in figure [[fig:hierarchical20]]. 

Two things should be noted about the results. First, although the priors on \theta are identical between this model and the model in section [[Multiple Parameters]], the posteriors differ. In the uniform case, for \theta_1 to \theta_{20}, the hierarchical model allows the posteriors to be more opinionated and more certain. This is a result of the learned abstract knowledge that the bags tend to be uniform in color, resulting in high confidence that the rest of the marbles drawn from any bag will be of that same color, too. 

Second, unlike the model in section [[Multiple Parameters]], the posterior of \theta_{21} differs between both cases different in mixed- and uniform case. Arguably in both cases, the estimate is a lot better: In particular, the result in the uniform case shows a strong tendency to expect the next marble out of \theta_{21} to be white, in accordance with the intuition about the uniform marble case.

** Forming Inductive Bias

To put it a bit more generally, a big selling point of HBMs is that they enable the learner to /transfer/ learned knowledge from one learning problem to another. To be sure, in the presented example case, there are other Bayesian options to account for the transfer of knowledge about the proportion of marbles from one bag to the next, for example by introducing more complicated models on the base level. HBMs, though, give a systematic account of how such a transfer can, in principle, be achieved. And they do so for arbitrary levels of abstraction. 

By providing this ability to transfer, HBMs give an astonishingly natural account of how inductive bias for a particular learning problem is acquired: by recognizing similarity between problems and transferring learned bias from an old problem to a new one. 

The attentive philosopher will note: This requires an inductive bias on a higher level, namely that similar problems should be addressed similarly, hence that knowledge on the object level is transferable. Doesn't this just push the problem of justifying inductive bias to a higher level, thereby not solving, but instead just displacing the problem of induction? We claim that is exactly what is happening, but that it is not a vice, but a virtue. It is similar to the prominent advantage of Bayesian models of for statistical purposes. They force us to be explicit about the prior information we bring into an inference problem. Similarly, HBMs make explicit the abstract knowledge involved in inductive reasoning. This way, Bayesian statistics just as HBMs in cognitive science open up their respective research problems to a whole new range of precise analysis. For the philosopher, too, HBMs open up the possibility to precisely analyze the inductive biases implicit in inductive reasoning problems, and may in this manner shed light on aspects of the bias that haven't been researched yet. So, definitely worth our time! 

** Hierarchical Bayesian Models of Cognition

The example model given above only shows that HBMs /can/ give a convincing account of abstract knowledge in a reasoning process. But this is exactly what we set out to do: Highlight HBMs as a possible way to model abstract knowledge. Of course, these cases here are over simplified. Rarely is data just binary without predictor variables. The levels of abstraction are trivial here, and amount to almost daunting proportions in realistic scenarios. To argue the claim that HBMs are a plausible model for many, if not almost all, cases of inductive reasoning with abstract knowledge is a whole research field. Helpful overviews are given by, e.g. citet:tenenbaum06_theor_based_bayes_model_induc_learn_reason,griffiths10_probab_model_cognit,tenenbaum11_how_to_grow_mind. HBMs are extremely flexible and can account for very diverse learning problems, as has been noted in the introduction. They can account for how such abstract knowledge is learned and formed, too citep:kemp10_probab_model_theor_format.

By showing how HBMs can plausibly model some inductive bias successfully, we have given a /prima facie/ reason to the claim that successful inductive inferences use abstract knowledge in a hierarchical structure to induce inductive bias. Of course, generalizing to inductive inferences in general is itself an inductive inference. We suggest to employ some hierarchically structured abstract background knowledge if one seeks to justify this further.


* Objections
** How Does Adding Parameters Reduce Complexity?

Making a model more complicated by adding additional parameters runs counter to the idea of introducing inductive bias, as these are concepts that are usually seen to be in opposition to one another, as exemplified in the bias-complexity-tradeoff problem (e.g. cite:shalev-shwartz14_under_machin_learn,harman07_reliab_reason). Instead, we might run the risk of overfitting. HBMs are undeniably more complicated than simple, flat models. We give two reasons that they nevertheless can still plausibly account for inductive biases in the intended sense. 

First, HBMs create a certain effect, called /shrinkage/ in the statistical literature citep:kruschke11_doing_bayes. In our bag example, the estimation of each \( \theta_i \) tends more towards the average than the data would suggest on its own, shrinking, in general, the range of estimations. This effect naturally reduces overfitting.

Second, the model applies to more data than a single model would. In the bag example, we treat all data within the same model with a lot of parameters, instead of having a separate model for each bag. Additionally, by also modeling more abstract knowledge, the model is open for more abstract data: receiving information that stacks of back tend to have the same characteristics, for example, can inform our model as higher-level data, so to speak. By accounting for more data, the risk to overfit is considerably lowered, as cite:perfors11_tutor_introd_to_bayes_model_cognit_devel, section 4, address.  

Taken together, by applying a single, more complex model to more situations and hence account for more data, hierarchical models /can/ induce biases and avoid increasing complexity.

** The Argument Rests on an Intuition 

As hinted at in section [[Multiple Parameters]], the central argument employs intuition about a case to argue for the use of HBMs. Given that we are talking about descriptive models, isn't this an obvious fallacy? We argue that is not, for multiple reasons. First, we are not making a descriptive claim. Rather, it is methodological, and hence may very well be valid without empirical foundations. Second, this is a philosophical paper, and recourse to intuitions about cases is standard in philosophy. Third, if that is not convincing, the intuition that you might or might not share actually has been found to be shared in empirical studies in similar cases citep:nisbett83_use_statis_heuris_every_induc_reason. 

** Human Reasoning Does not Guarantee Rational Reasoning

Perhaps it needs to be added that the claim in this paper is /not/ meant to imply that we just look at how humans reason, model that hierarchically, and voila, solve the problem of induction. Obvious /non sequitur/ aside, it is provably very often the case that humans do not reason rationally. We might even find that most human reasoning is not optimal. Instead, we want to look at specific cases that highlight the human mind's extraordinary ability to generalize successfully from sparse data, as described in the beginning.  

* Conclusion

By giving a systematic account of how inductive bias can be and plausibly is successfully acquired in many human learning situations, HBMs inform how, reasonably, abstract background knowledge can be applied to inductive inference problems. While providing no obvious solution to Hume`s original problem of induction, we certainly gain insight into rationally structured inductive bias in human reasoning, and, by extension, into the nature of bias in learning problems.

* Appendix

The hierarchical models described in this paper and the results presented have been implemented with the programming language R and a sampling framework called JAGS, which implements a type of Gibbs sampling Markov Chain Monte Carlo. Figures [[fig:flat20]] and [[fig:hierarchical20]] have been generated by 4 Markov chains with 200.000 iterations each. Some minor auxiliary functionality was used from the accompanying program code of citet:kruschke11_doing_bayes. All source code can be reviewed at the author's git repository under 

https://github.com/kurtfritz/bayesian-cognition.

\printbibliography

